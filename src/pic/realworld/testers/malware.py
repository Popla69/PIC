"""Malicious Pattern Recognition Testing

Tests PIC's ability to detect known malware behavior patterns in safe mode.
"""

import time
import logging
from pathlib import Path
from typing import Dict, Any
import tempfile
import os

from pic.cellagent import CellAgent
from pic.realworld.safety import SafetyController


class MalwarePatternTester:
    """Tests PIC's malware pattern detection capabilities.
    
    This tester validates that PIC can detect:
    - File wiper patterns (mass file operations)
    - Reverse shell patterns (network-like behavior)
    - Keylogger patterns (input monitoring)
    - Sandbox containment
    - Forensic information logging
    
    All tests use safe, educational variants that don't cause actual harm.
    """
    
    def __init__(self, agent: CellAgent, safety: SafetyController):
        """Initialize malware pattern tester.
        
        Args:
            agent: CellAgent instance to test
            safety: SafetyController for validation
        """
        self.agent = agent
        self.safety = safety
        self.logger = logging.getLogger(__name__)
    
    def test_file_wiper_pattern(self, sandbox_path: Path) -> Dict[str, Any]:
        """Test detection of file wiper patterns.
        
        Args:
            sandbox_path: Sandbox directory for safe file operations
            
        Returns:
            Test result dictionary
        """
        self.logger.info("Testing file wiper pattern detection...")
        
        @self.agent.monitor
        def file_operation(operation_type: str, file_count: int) -> int:
            """Simulated file operation."""
            # Simulate time based on operation count
            time.sleep(file_count * 0.001)
            return file_count
        
        # Establish baseline with normal file operations
        for i in range(30):
            file_operation("normal_write", 1)
        
        # Simulate file wiper pattern (mass operations)
        wiper_file_count = 100  # 100 files at once
        result = file_operation("mass_rename", wiper_file_count)
        
        # Detection: mass file operations should be flagged
        detected = result is None
        
        return {
            "passed": detected,
            "detections": 1 if detected else 0,
            "metrics": {
                "baseline_file_count": 1,
                "wiper_file_count": wiper_file_count,
                "operation_rate_multiplier": 100,
                "detected": detected
            },
            "forensic_data": {
                "attack_type": "file_wiper",
                "pattern": "mass_file_operations",
                "sandbox_path": str(sandbox_path)
            }
        }
    
    def test_reverse_shell_pattern(self) -> Dict[str, Any]:
        """Test detection of reverse shell patterns.
        
        Returns:
            Test result dictionary
        """
        self.logger.info("Testing reverse shell pattern detection...")
        
        @self.agent.monitor
        def network_operation(operation_type: str, frequency: int) -> str:
            """Simulated network operation."""
            # Simulate network-like timing
            time.sleep(frequency * 0.002)
            return f"operation_{operation_type}"
        
        # Establish baseline with normal operations
        for i in range(30):
            network_operation("normal_request", 1)
        
        # Simulate reverse shell pattern (rapid network-like operations)
        shell_frequency = 50  # High frequency
        result = network_operation("shell_connect", shell_frequency)
        
        # Detection: reverse shell pattern should be flagged
        detected = result is None
        
        return {
            "passed": detected,
            "detections": 1 if detected else 0,
            "metrics": {
                "baseline_frequency": 1,
                "shell_frequency": shell_frequency,
                "frequency_multiplier": 50,
                "detected": detected
            },
            "forensic_data": {
                "attack_type": "reverse_shell",
                "pattern": "high_frequency_network_operations"
            }
        }
    
    def test_keylogger_pattern(self) -> Dict[str, Any]:
        """Test detection of keylogger patterns.
        
        Returns:
            Test result dictionary
        """
        self.logger.info("Testing keylogger pattern detection...")
        
        @self.agent.monitor
        def input_monitoring(monitor_type: str, capture_rate: int) -> int:
            """Simulated input monitoring."""
            # Simulate monitoring overhead
            time.sleep(capture_rate * 0.001)
            return capture_rate
        
        # Establish baseline with normal input handling
        for i in range(30):
            input_monitoring("normal_input", 1)
        
        # Simulate keylogger pattern (continuous monitoring)
        keylog_rate = 100  # High capture rate
        result = input_monitoring("continuous_capture", keylog_rate)
        
        # Detection: keylogger pattern should be flagged
        detected = result is None
        
        return {
            "passed": detected,
            "detections": 1 if detected else 0,
            "metrics": {
                "baseline_rate": 1,
                "keylog_rate": keylog_rate,
                "rate_multiplier": 100,
                "detected": detected
            },
            "forensic_data": {
                "attack_type": "keylogger",
                "pattern": "continuous_input_monitoring"
            }
        }
    
    def test_sandbox_containment(self, sandbox_path: Path) -> Dict[str, Any]:
        """Test that malicious operations are contained in sandbox.
        
        Args:
            sandbox_path: Sandbox directory
            
        Returns:
            Test result dictionary
        """
        self.logger.info("Testing sandbox containment...")
        
        # Try to create files in sandbox
        test_file = sandbox_path / "data" / "test_malware.txt"
        
        try:
            # This should succeed (within sandbox)
            test_file.parent.mkdir(parents=True, exist_ok=True)
            test_file.write_text("safe test data")
            
            # Verify file is in sandbox
            contained = self.safety.validate_file_path(test_file)
            
            # Try to access outside sandbox (should fail)
            outside_path = Path("/tmp/outside_sandbox.txt")
            outside_contained = not self.safety.validate_file_path(outside_path)
            
            # Cleanup
            if test_file.exists():
                test_file.unlink()
            
            passed = contained and outside_contained
            
            return {
                "passed": passed,
                "metrics": {
                    "sandbox_access_allowed": contained,
                    "outside_access_blocked": outside_contained,
                    "sandbox_path": str(sandbox_path)
                },
                "forensic_data": {
                    "test_type": "sandbox_containment",
                    "files_created": 1 if contained else 0
                }
            }
            
        except Exception as e:
            self.logger.error(f"Sandbox containment test error: {e}")
            return {
                "passed": False,
                "error": str(e),
                "metrics": {}
            }
    
    def test_forensic_logging(self) -> Dict[str, Any]:
        """Test that malicious patterns generate forensic logs.
        
        Returns:
            Test result dictionary
        """
        self.logger.info("Testing forensic logging...")
        
        @self.agent.monitor
        def malicious_operation(operation_id: str, severity: str) -> str:
            """Operation that should generate forensic logs."""
            if severity == "high":
                time.sleep(0.1)  # Simulate malicious delay
            return f"operation_{operation_id}"
        
        # Establish baseline
        for i in range(30):
            malicious_operation(f"normal_{i}", "low")
        
        # Execute malicious operation
        result = malicious_operation("malicious_001", "high")
        
        # Detection means forensic data should be logged
        detected = result is None
        
        # Check if safety controller recorded the event
        safety_stats = self.safety.get_statistics()
        
        return {
            "passed": detected,
            "detections": 1 if detected else 0,
            "metrics": {
                "forensic_data_generated": detected,
                "safety_violations": safety_stats.get("violations", 0)
            },
            "forensic_data": {
                "operation_id": "malicious_001",
                "severity": "high",
                "timestamp": time.time(),
                "detection_method": "behavioral_analysis"
            }
        }
    
    def run_all_tests(self, sandbox_path: Path) -> Dict[str, Any]:
        """Run all malware pattern tests.
        
        Args:
            sandbox_path: Sandbox directory for safe operations
            
        Returns:
            Comprehensive test results
        """
        self.logger.info("Running all malware pattern tests...")
        
        results = {
            "file_wiper": self.test_file_wiper_pattern(sandbox_path),
            "reverse_shell": self.test_reverse_shell_pattern(),
            "keylogger": self.test_keylogger_pattern(),
            "sandbox_containment": self.test_sandbox_containment(sandbox_path),
            "forensic_logging": self.test_forensic_logging()
        }
        
        # Calculate overall metrics
        total_tests = len(results)
        passed_tests = sum(1 for r in results.values() if r.get("passed", False))
        total_detections = sum(r.get("detections", 0) for r in results.values())
        
        return {
            "test_category": "malware_pattern",
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "pass_rate": passed_tests / total_tests,
            "total_detections": total_detections,
            "individual_results": results,
            "metrics": {
                "tests_passed": passed_tests,
                "tests_failed": total_tests - passed_tests
            }
        }
